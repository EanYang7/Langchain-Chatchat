## 使用自定义的分词器
1. 在```text_splitter```文件夹下新建一个文件，文件名为您的分词器名字，比如`my_splitter.py`，然后在`__init__.py`中导入您的分词器，如下所示：
```python
from .my_splitter import MySplitter
```

2. 修改```config/model_config.py```文件，将您的分词器名字添加到```text_splitter_dict```中，如下所示：
```python
MySplitter: {
        "source": "huggingface",  # 选择tiktoken则使用openai的方法
        "tokenizer_name_or_path": "your tokenizer", #如果选择huggingface则使用huggingface的方法，部分tokenizer需要从Huggingface下载
    }
TEXT_SPLITTER = "MySplitter"
```

完成上述步骤后，就能使用自己的分词器了。

## 使用自定义的 Agent 工具

1. 创建自己的Agent工具

+ 开发者在```server/agent```文件中创建一个自己的文件，并将其添加到```tools.py```中。这样就完成了Tools的设定。

+ 当您创建了一个```custom_agent.py```文件，其中包含一个```work```函数，那么您需要在```tools.py```中添加如下代码：
```python
from custom_agent import work
Tool.from_function(
    func=work,
    name="该函数的名字",
    description=""
    )
```
+ 请注意，如果你确定在某一个工程中不会使用到某个工具，可以将其从Tools中移除，降低模型分类错误导致使用错误工具的风险。

+ 如果您创建的Agent工具是为了给```ChatGLM3-6B```模型使用的，您应该单独创建一个与工具名称相同的yaml文件。
请注意，是与Tools中的```name```相同，而不是执行函数的名字。创建的yaml文件中，应该与GLM3官方仓库中的格式完全一致。

2. 修改 ```custom_template.py``` 文件

开发者需要根据自己选择的大模型设定适合该模型的Agent Prompt和自自定义返回格式。
````
"""
Answer the following questions as best you can. You have access to the following tools:
{tools}
Use the following format:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can be repeated zero or more times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
Begin!
history:
{history}
Question: {input}
Thought: {agent_scratchpad}
"""
````
除了使用 `Zero React` 的提示词方案，开发者可以自行对提示词进行修改，或者使用 Langchain 提供的其他的Agent结构。例如，如果您使用的模型为`ChatGLM3-6B`模型，我们提供了一个可以正常运行`ChatGLM3-6B`的Agent提示词，该提示词与 Langchain 的 `struct Agent`相似，其内容如下：
````

"ChatGLM3":
"""
You can answer using the tools, or answer directly using your knowledge without using the tools.Respond to the human as helpfully and accurately as possible.
You have access to the following tools:
{tools}
Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).
Valid "action" values: "Final Answer" or  [{tool_names}]
Provide only ONE action per $JSON_BLOB, as shown:

```
{{{{
  "action": $TOOL_NAME,
  "action_input": $INPUT
}}}}
```
Follow this format:

Question: input question to answer
Thought: consider previous and subsequent steps
Action:
```
$JSON_BLOB
```
Observation: action result
... (repeat Thought/Action/Observation N times)
Thought: I know what to respond
Action:
```
{{{{
  "action": "Final Answer",
  "action_input": "Final response to human"
}}}}
Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.

history: {history}

Question: {input}

Thought: {agent_scratchpad}
""",
````

3. 让不支持 Langchain 调用方式的但具备 Agent 能力的模型展现能力

以**ChatGLM3-6B**为代表的模型，虽然具有 Function Call 能力，但其对齐格式与 Langchain 提供默认Agent格式并不符合，因此无法使用 Langchain 自身能力实现 Function Call。在我们的框架中，您可以在 ```server/Agent/custom_agent/``` 文件夹中自行复现更多模型的 Agent 能力实现。

在完成上述步骤之后，您还需要到```server/chat/agent_chat/```中导入您的模块来实现特殊判定。

4. 局限性

- 在我们的实验中，GPT4能够稳定的发挥出Agent能力，Qwen具有基础的Agent能力。Qwen系列模型已经对 Agent 进行了对齐，但是经过开发组调试，其效果尚且不能完成大规模的Agent任务。__我们不推荐__ 开发者花费无用的时间在尝试未经过微调的其他模型。
- 由于 React Agent 的脆弱性，temperature 参数的设置对于模型的效果有很大的影响。我们建议开发者在使用自定义 Agent 时，对于不同的模型，将其设置成0.1以下，以达到更好的效果。
- 目前，官方仅对 **ChatGLM3-6B** 一种模型进行了 非 Langchain 对齐格式下的能力激活，我们欢迎开发者自行探索其他模型，并提交对应的 PR，让框架支持更多的 Agent 模型。
- 基于 Langchain 的 Agent 架构与 `AutoGen` 等新一代 Agent 架构具有一定区别，因此，我们目前的主要实现仍保持在工具调用阶段。

## 使用自定义的微调模型

- 本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 PEFT 路径。
- 开发者需要保证路径名称里必须有 peft 这个词。
- 配置文件的名字为 ```adapter_config.json```
- peft 路径下包含.bin 格式的 PEFT 权重， peft路径在startup.py中 ```create_model_worker_app``` 函数的 ```args.model_names``` 中指定
```python
    args.model_names = ["/home/ubuntu/your_peft_folder/peft"]

```
- 执行代码之，应该设定环境变量
```
PEFT_SHARE_BASE_WEIGHTS=true 
```

注：如果上述方式启动失败，则需要以标准的 FastChat 服务启动方式分步启动，PEFT加载详细步骤参考以下ISSUE

[加载lora微调后模型失效](https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822)

在```最佳实践```章节中，我们为开发者做了更详细的模型载入文档。

__该功能可能还具有一定的Bug，需要开发者仔细适配。__


## 使用自定义的嵌入模型

- 使用自定义的嵌入模型，开发者需要将其合并到原始的嵌入模型中，之后仅需将其路径添加到```config/model_config.py```中并选择自己的模型启动即可。
- 如果想自己在Embedding模型中支持 自定义的关键字，需要在 ```embeddings/embedding_keywords.txt```中设定好自己的关键字
- 运行 ```embeddings/add_embedding_keywords.py```
- 将生成的新Embedding模型地址放入```configs/model_config.py```中并选择，
```
"custom-embedding": "your path",
```
并设置
```
EMBEDDING_MODEL = "custom-embedding"  
```
即可调用加入关键字的embedding模型。
在```最佳实践```章节中，我们为某几个关键词定制了一个Embed模型。

## 日志功能

- 日志功能记录了大模型的心跳和网络端口传输记录，开发者可以通过日志功能查看模型的运行情况。